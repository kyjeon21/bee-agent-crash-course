import "dotenv/config";
import { MilvusClient } from "@zilliz/milvus2-sdk-node";
import { EmbeddingModel } from "bee-agent-framework/backend/embedding";
import {
  BaseAgent,
  BaseAgentRunOptions,
} from "bee-agent-framework/agents/base";
import {
  AssistantMessage,
  Message,
  SystemMessage,
  UserMessage,
} from "bee-agent-framework/backend/message";
import { Emitter } from "bee-agent-framework/emitter/emitter";
import { GetRunContext } from "bee-agent-framework/context";
import { z } from "zod";
import { AgentMeta } from "bee-agent-framework/agents/types";
import { BaseMemory } from "bee-agent-framework/memory/base";
import { UnconstrainedMemory } from "bee-agent-framework/memory/unconstrainedMemory";
import { ChatModel } from "bee-agent-framework/backend/chat";
import { WatsonxChatModel } from "bee-agent-framework/adapters/watsonx/backend/chat";
import { createConsoleReader } from "../../helpers/io.js";


const milvusConfig = {
  address: process.env.MILVUS_ADDRESS,
  username: process.env.MILVUS_USERNAME,
  password: process.env.MILVUS_PASSWORD,
  ssl: process.env.MILVUS_SSL === "true",
};

const COLLECTION_NAME = "hackathon_newsdata_20250215_dev";

interface RunInput {
  message: Message;
}

interface RunOutput {
  message: Message;
  state: {
    thought: string;
    final_answer: string;
    follow_up_needed: boolean;
    evaluation?: { score: number; feedback: string };
  };
}

interface RunOptions extends BaseAgentRunOptions {
  maxRetries?: number;
}

interface AgentInput {
  llm: ChatModel;
  memory: BaseMemory;
  milvusClient: MilvusClient;
  embModel: EmbeddingModel;
}

export class VectorAgent extends BaseAgent<RunInput, RunOutput, RunOptions> {
  public readonly memory: BaseMemory;
  protected readonly model: ChatModel;
  protected readonly milvusClient: MilvusClient;
  protected readonly embModel: EmbeddingModel;
  public emitter = Emitter.root.child({
    namespace: ["agent", "vector"],
    creator: this,
  });

  constructor(input: AgentInput) {
    super();
    this.model = input.llm;
    this.memory = input.memory;
    this.milvusClient = input.milvusClient;
    this.embModel = input.embModel;
  }

  /**
   * ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ ë¬¸ì„œë¥¼ ê²€ìƒ‰
   */
  private async vectorSearch(query: string) {
    // 1. ì…ë ¥ì„ ë²¡í„°í™”
    const embedding = await this.embModel.create({ values: [query] });
    const searchVector = embedding.embeddings[0];

    // 2. Milvusì—ì„œ ê²€ìƒ‰ ì‹¤í–‰
    const searchResponse = await this.milvusClient.search({
      collection_name: COLLECTION_NAME,
      vector: searchVector,
      params: { nprobe: 64 },
      limit: 3,
      metric_type: "COSINE",
      output_fields: ["id", "content", "metadata"],
    });

    // 3. ê²°ê³¼ ì •ì œ (ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œê±°)
    return searchResponse.results.map((result) => {
      if (result.metadata) {
        delete result.metadata.dialogue;
        delete result.metadata.split_content;
      }
      return result;
    });
  }

  /**
   * ì‹¤í–‰ ë¡œì§ (_run)
   */
  protected async _run(
    input: RunInput,
    options: RunOptions & { iteration?: number },
    run: GetRunContext<this>
  ): Promise<RunOutput> {
    // ê¸°ë³¸ì ìœ¼ë¡œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ 3ìœ¼ë¡œ ì„¤ì •
    const iteration = options.iteration ?? 1;
    if (iteration > 3) {
      const summaryResponse = await this.model.createStructure({
        schema: z.object({
          summary: z.string().describe("A summary of the responses generated so far and an indication that 3 iterations have been reached"),
        }),
        messages: [
          new SystemMessage(`
    Output ONLY in JSON format. Your response must strictly follow this format:
    {
      "summary": "Your summary message in English."
    }
    Do NOT include any additional text, explanations, or markdown formatting.
          `),
          new UserMessage(
            `We have reached 3 iterations. Please provide a concise summary of the responses so far and note that the maximum iteration count has been reached.`
          ),
        ],
        maxRetries: 1,
      });
    
      const summaryOutput = summaryResponse.object as { summary: string };
    
      // Return the summary as the final response
      const resultMessage = new AssistantMessage(summaryOutput.summary);
      await this.memory.add(resultMessage);
    
      return {
        message: resultMessage,
        state: {
          thought: "Reached the maximum iteration count (3).",
          final_answer: summaryOutput.summary,
          follow_up_needed: false,
          evaluation: { score: 0, feedback: "Iteration limit reached." },
        },
      };
    }

    // ì‚¬ìš©ì ì…ë ¥ì—ì„œ queryText ì¶”ì¶œ
    const userQuery = (input.message as UserMessage).content as any;
    const queryText =
      typeof userQuery === "string"
        ? userQuery
        : typeof userQuery === "object" &&
          userQuery !== null &&
          "text" in userQuery
        ? (userQuery.text as string)
        : Array.isArray(userQuery)
        ? userQuery
            .map((part) =>
              typeof part === "string"
                ? part
                : typeof part === "object" && part !== null && "text" in part
                ? (part.text as string)
                : JSON.stringify(part)
            )
            .join(" ")
        : JSON.stringify(userQuery);
    console.log("ğŸ“ Processed Query:", queryText);

    const searchResults = await this.vectorSearch(queryText);
    console.log(
      "ğŸ” Milvus Search Results:",
      JSON.stringify(searchResults, null, 2)
    );

    // ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    const response = await this.model.createStructure({
      schema: z.object({
        thought: z.string().describe("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒê°"),
        final_answer: z.string().describe("ìµœì¢… ë‹µë³€"),
        follow_up_needed: z.boolean().describe("ì¶”ê°€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€"),
      }),
      messages: [
        new SystemMessage(`
Output ONLY in JSON format. Your response must strictly follow this format:
{
  "thought": "Your thought process based on the search results",
  "final_answer": "Your final answer",
  "follow_up_needed": true or false
}
Do NOT include any additional text, explanations, or markdown formatting.
        `),
        new UserMessage(
          `User Query: "${queryText}"\n\nRelevant Documents:\n${JSON.stringify(
            searchResults,
            null,
            2
          )}`
        ),
      ],
      maxRetries: options?.maxRetries,
      abortSignal: run.signal,
    });

    // ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
    console.log("ğŸ’¬ ìƒì„±ëœ ë‹µë³€ ì‘ë‹µ:", response);

    const output = response.object as {
      thought: string;
      final_answer: string;
      follow_up_needed: boolean;
    };

    // í‰ê°€ ë‹¨ê³„: LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ í‰ê°€ (0-100 ì ê³¼ í”¼ë“œë°± ì„¤ëª… ìƒì„±)
    const evaluation = await this.model.createStructure({
      schema: z.object({
        score: z.number().int().min(0).max(100),
        feedback: z.string(),
      }),
      messages: [
        new SystemMessage(`
Strictly adhere to the following rules:
1. Output must be a SINGLE JSON object.
2. The response must start with '{' and end with '}'.
3. Only include the keys "score" and "feedback" in the specified order.
4. "feedback" must be concise (max 100 characters).
5. Do NOT add any additional text, comments, or markdown.

Scoring Guidelines:
- Assign a score between 0-100. If the response is not perfect, do not give more than 95.
- If the answer is logical but has room for improvement, assign a score between 60-90.
- If there are factual inaccuracies, assign a score below 60.
- Only provide 95+ for truly flawless responses.

You are a STRICT evaluator. If any part of the response is lacking, assign a lower score.
        `),
        new UserMessage(`Query: "${queryText}"`),
        new UserMessage(`Answer: "${output.final_answer}"`),
      ],
    });

    const evalResult = evaluation.object as { score: number; feedback: string };
    console.log("ğŸ“Š í‰ê°€ ê²°ê³¼:", evalResult);

    // í‰ê°€ ì ìˆ˜ê°€ 80 ì´ìƒì´ë©´ ìµœì¢… ë‹µë³€ì„ ë°˜í™˜
    if (evalResult.score >= 95) {
      const resultMessage = new AssistantMessage(output.final_answer);
      await this.memory.add(resultMessage);
      return {
        message: resultMessage,
        state: {
          ...output,
          evaluation: evalResult,
        },
      };
    } else {
      const maxFeedbackLength = 100;
      const shortFeedback =
        evalResult.feedback.length > maxFeedbackLength
          ? evalResult.feedback.slice(0, maxFeedbackLength) + "..."
          : evalResult.feedback;

      // LLMì„ í˜¸ì¶œí•´ì„œ ìƒˆ ì§ˆì˜ë¥¼ ìƒì„±
      const refinementResponse = await this.model.createStructure({
        schema: z.object({
          refined_query: z.string(),
        }),
        messages: [
          new SystemMessage(`
Output must be a SINGLE JSON object only.
{
  "refined_query": "Revised search query"
}
Do NOT include any additional text, comments, paragraphs, or markdown formatting.
    `),
          new UserMessage(`Feedback: ${shortFeedback}`),
          new UserMessage(`Original Answer: ${output.final_answer}`),
        ],
        maxRetries: 2,
        abortSignal: run.signal,
      });

      const refinedOutput = refinementResponse.object as {
        refined_query: string;
      };
      const refinedQuery = refinedOutput.refined_query;
      console.log("ğŸ”„ ìƒì„±ëœ ì¬ì‘ì„± ì§ˆì˜:", refinedQuery);

      // ì¬ê·€ í˜¸ì¶œ ì „ì— ë©”ëª¨ë¦¬ë¥¼ í´ë¦¬ì–´í•˜ì—¬ íˆìŠ¤í† ë¦¬ ëˆ„ì ì„ ë°©ì§€í•©ë‹ˆë‹¤.
      // if (typeof this.memory.clear === "function") {
      //   await this.memory.clear();
      // }

      // ì¬ê·€ í˜¸ì¶œ ì‹œ iteration ì¦ê°€
      return this._run(
        { message: new UserMessage(refinedQuery) },
        { ...options, iteration: iteration + 1 },
        run
      );
    }
  }

  public get meta(): AgentMeta {
    return {
      name: "VectorAgent",
      description:
        "An AI agent that performs vector search and iteratively refines queries.",
      tools: [],
    };
  }

  createSnapshot() {
    return {
      ...super.createSnapshot(),
      emitter: this.emitter,
      memory: this.memory,
    };
  }

  loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>) {
    Object.assign(this, snapshot);
  }
}

/**
 * ì—ì´ì „íŠ¸ ì‹¤í–‰
 */
// (async () => {
//   const milvusClient = new MilvusClient(milvusConfig);
//   await milvusClient.connectPromise;
//   console.log("âœ… Connected to Milvus successfully.");

//   const embeddingModel = await EmbeddingModel.fromName(
//     "watsonx:intfloat/multilingual-e5-large"
//   );

//   const model = await WatsonxChatModel.fromName(
//     "watsonx:meta-llama/llama-3-3-70b-instruct"
//   );

//   // LLM ëª¨ë¸ ì„¤ì •
//   model.config({
//     parameters: {
//       maxTokens: 9000,
//       temperature: 0.0,
//     },
//   });

//   // ì—ì´ì „íŠ¸ ìƒì„±
//   const vectorAgent = new VectorAgent({
//     llm: model,
//     memory: new UnconstrainedMemory(),
//     milvusClient: milvusClient,
//     embModel: embeddingModel,
//   });

//   // ì‹¤í–‰ í…ŒìŠ¤íŠ¸
//   const response = await vectorAgent.run(
//     {
//       message: new UserMessage(
//         "What is the main topic of the article 'Philadelphia Preps for Pricey Winter Season?'"
//       ),
//     },
//     { maxRetries: 3 }
//   );

//   console.log("ğŸ¤– Final Response:", response.state);
// })();


(async () => {
  const milvusClient = new MilvusClient(milvusConfig);
  await milvusClient.connectPromise;
  console.log("âœ… Connected to Milvus successfully.");

  const embeddingModel = await EmbeddingModel.fromName("watsonx:intfloat/multilingual-e5-large");
  const model = await WatsonxChatModel.fromName("watsonx:meta-llama/llama-3-3-70b-instruct");
  model.config({ parameters: { maxTokens: 9000, temperature: 0.0 } });

  const vectorAgent = new VectorAgent({
    llm: model,
    memory: new UnconstrainedMemory(),
    milvusClient: milvusClient,
    embModel: embeddingModel,
  });

  let lastResult: { final_answer?: string } = {}; // íƒ€ì…ì„ ëª…í™•íˆ ì§€ì •
  const reader = createConsoleReader();

  for await (const { prompt } of reader) {
    const response = await vectorAgent.run(
      { message: new UserMessage(prompt) },
      { maxRetries: 3 }
    );

    lastResult = response.state || {}; // ë§Œì•½ stateê°€ undefinedì´ë©´ ë¹ˆ ê°ì²´ í• ë‹¹
    reader.write("ğŸ¤– Answer", lastResult.final_answer ?? "No response available.");
  }
})();
